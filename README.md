---
output:
  html_document:
    toc: yes
---


# Using this Package

## System Requirements
This project makes use of the [STAR aligner](https://github.com/alexdobin/STAR)
to sequence human genome data.  STAR requires more than 30G of RAM for this
purpose.  In general genome sequencing carries relatively high demands for
compute, RAM, and disk I/O resources (but not for graphics resources, at
least not with the software used in this project).

This project also makes use of various resource-intensive R packages.

Nonetheless, the necessary hardware is attainable in high-end
consumer-grade systems.

### Description of Development Systems
The following systems were used during development of this work:

1. System76 "Serval" mobile workstation
    * Intel i7-8700k CPU
    * 64GB RAM (DDR4-3000, non-ECC)
    * Samsung NVMe Pro SSD
    * Pop!_OS 17.10
2. PowerSpec G460 desktop computer
    * Intel i7-8700k CPU
    * 48GB RAM (DDR4-3200, non-ECC)
    * Intel M.2 SATA SSD
    * Windows 10 Pro ; Windows Subsystem for Linux, "WLinux" flavor

Note that WSL is still evolving and rough around the edges.  For example,
the "atom" editor has some bugs that prevent plug-in installation under WSL.

In Q4 2018, Apple announced a new Mac Mini lineup.  A fully upgraded Mini
seems to have specs comparable to the development systems and in theory
should be suitable, but no attempt has been made to evaluate this.  Note
that some adjustments to workflows may be necessary on MacOS.  For example,
Mac OS does not make the 'wget' utility available by default, nor do they
make it trivially easy to install (there are several options for
installing it, but they are not trivial in the sense that 'sudo apt-get install'
is trvial).

### Performance of Development Systems

System (1) is capable, starting from scratch, to download and compile
the STAR source code, download human genome data, align a reference
human genome, and translate 8 .fastq.gz files into .bam counterparts,
all in about 75 minutes.

System (2) is capable, starting from scratch, to 'make all' in about 4 hours.

The "Million of reads per hour" figures recorded in `*Log.final.out` files
generated by STAR, and summarized by R, indicate the following alignment
performance for the development systems:
```
          Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
System 1 336.4   347.5   352.1   357.8   367.7   380.0
System 2 288.0   296.4   314.6   312.9   324.9   348.0
```

The average rates of 357.8 and 312.9 may be compared to the
[original published STAR performance](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/),
which indicates 309.2 when using 6 cores, and 549.9 when using 12 cores,
on a high-performance server in 2012.


## Installing Prerequisite Software Packages
This software assumes that various tools are already available in your
environment.  If you have not done so already, it will be useful for you to do
the following (for Debian-based environments):

* `sudo apt-get install bamtools`
* `sudo apt-get install bedtools`
* `sudo apt-get install git-lfs  # needed for large .fastq raw data files`
* `sudo apt-get install libssl-dev  # needed by various R packages`
* `sudo apt-get install macs`
* `sudo apt-get install picard-tools  # needed for insert size analysis`
* `sudo apt-get install r-base`
* `sudo apt-get install r-recommended`
* `sudo apt-get install samtools`

## Cloning this Package from Github

In a directory of your choosing, execute:

`git clone git@github.com:a3609640/DNFA-gene-analysis.git`

Note, both here and in "Configuring Your Local Environment", that you
may wish to consider locations carefully.  In particular, you should
make sensible plans for file system free space, and for underlying disk
speed.

## Configuring Your Local Environment
In the top-level directory of your Git clone, there is a file named
'configuration.tmpl'.  You should copy this file locally to one named
'configuration', edit the contents as you prefer (to indicate local directories
where generated files will be stored, and the like), and execute the commmand
`source configuration` before using the scripts in this package.  That will
ensure that necessary shell environment variables are set.

## First Steps

From the top-level directory of your Git project clone, the first command to
invoke, after `source configuration`, is `make`.

That will execute all workflows associated with the project (the workflows
are described in detail below).  For the 'asospecificity' workflow in particular,
it will download the sources for a specific version of STAR, and
compile them on your machine.  Then it will download and sequence a reference
human genome.  Then it will process various project-specific files against That
reference genome, ultimately converting them from .fastq.gz to .bam format.

On System (2) above, this step takes about three hours.

The Makefile attempts to Do The Right Thing if it is invoked multiple times.
It creates output directories with sensible permissions if they are absent.
If STAR is already built and installed it does not repeat the download and
compilation.  If human genome files are already downloaded it does not
download them again.  If a reference genome has already been sequenced,
it is reused.  Etc.

However note that the detection of previously-completed work is not
foolproof.  If in doubt, it is best to `make clean` and re-execute.
Note that there are also more fine-grained 'clean' targets, to permit
isolated re-exeuctions of portions of thr processing pipeline.


# Workflows Provided by This Project

This Makefile permits fine-grained control over multiple full processing
pipelines.  The only required input is a .tar file with that contains all
the raw .fastq.gz data (which are now also archived with NIH GEO under
accession number GSE122707).

Everything else can be generated from scratch, given that file and an
internet connection.

The commands `make` or `make all` will build everything, from soup to nuts.

The following specific workflows are also supported:

## DNFA gene expression analysis with TCGA and GTEx databases.
This depends on Makefile, R/CGDSR.R, R/GTEx.R, R/Xena.R

Specific targets associated with this workflow include the following:
* `make genedbanalysis` -- downloads public GTEX data and annotations
    to the r-extdata directory under the directory named by they
    configured environment variable `DNFA_generatedDataRoot`.

For the time being, the R scripts must still be manually invoked after
the data is downloaded.

## DNFA gene expression analysis on single cell RNA-seq dataset
This depends on Makefile and R/scRNA-Seq.R.

Specific targets associated with this workflow include the following:

* `make singlecellanalysis` -- downloads a public single-cell data
    set to the r-extdata directory under the directory named by the
    configured environment variable `DNFA_generatedDataRoot`.

The script R/scRNA-Seq.R will search for the file in the same location
where the Makefile deposits it, provided that it is executed with the
same environment.  For the time being, the .R file must be evaluated
independently of any Makefile target.

## RNA-seq analysis for ASO reagent specificity
This workflow depends upon Makefile, R/DESeqDataPreparation.R,
and DESeqandFactoMineRforASOSpecificity.R.  The Makefile
is capable of executing the workflow through the generation
of a gene-counts.csv file.  That file is used as input for
R scripts, which must (for the time being) be executed
separately.  Note that two versions of the gene-counts.csv
file are checked into GitHub under this project's project-data
directory.

Specific targets associated with this workflow include the following:

* `make asoanalysis` -- executes the entire workflow

* `make bamfiles` -- The generated .bam files (and their output directory)
    may be removed with `make clean_bamfiles`.

* `make samfiles` -- Build everything but stop short of the 'samtools sort'
    step.  The generated .sam files (and their output directory) may be
    removed with `make clean_samfiles`.

* `make reference_genome` -- Build STAR, then download human genome data and
    create a reference genome.  Do not attempt to process any .fastq.gz data
    from the project-specific .tar data file.  The reference genome *data*
    (and its output directory) may be removed with `make clean_refgenome`.
    The reference genome *alignment* (and its output directory) may be
    removed with `make clean_starindex`.

* `make ${DNFA_starRoot}/STAR-2.6.1a/bin/STAR` -- Download STAR sources
    (if needed), compile them, and install them in
    ${DNFA_starRoot}/STAR-2.6.1a/bin.  STAR may be removed with
    `make clean_star`.

* In addition, generally, individual .sam, .bam, and .fastq.gz files may
    be generated with `make [filename]`, provided that the full filename
    in the generated data directories specified in 'configuration' is
    known.

## Pathway analyses for RNA-Seq and ChIP-Seq analyses

This depends on: ChIP-Seq/MACSandHOMERforChIP-Seq.sh, R/ChIPseeker.R,
R/DESeqandGAGEforPathwayAnalysis.R.

Specific targets associated with this workflow include the following:

* `make chipseqanalysis` -- downloads multiple data files from existing
    public data sets, and processes them into various .sam, .bam, and .bed
    files
* `make <download_dir/download_filename>` -- for various files (see the
    Makefile itself), this not only downloads a specific ENCFF*.bam file,
    but also verifies its md5 checksum.

For the time being, R scripts must be manually invoked to generate plots
from the downloaded and processed data.


# Software Engineering Discussion for Biologists

As a friend of one of the authors who was a member of the US Marine Corps
once related, there is a saying in the military:  "Amateurs talk tactics;
professionals talk logistics."  In a way, the same is true for software
engineering.  If you have managed to coax R into producing a
publication-quality graph, congratulations... but from a software engineering
standpoint, you are far from done.  Is your code correct?  If you adjust it,
how can you be sure that it remains correct?  Can your software run in a fully
automated fashion with no manual steps?  What recipe must others follow to see
your code work in their own environments?  These and related questions touch
upon the logistical considerations for well-engineered software.  This section
will consider them from the standpoint of build automation, unit testing, and
documentation.

## Build Automation

One aspect of this project is a file named `Makefile`.  To a non-initiate,
its contents undoubtedly appear as though, periodically during its creation,
a cat walked across a keyboard with a stuck `shift` key, taking particular
care to step only on the number keys.  However the file in fact defines
many specific buildable units, the relationships between them, and precise
recipes to follow in order to build them.

### A Primer on Makefiles

Putting aside the meanings of various symbols, the basic idea is
straightforward.  The Makefile provides a set of rules with the following
form (as described in the
[documentation for GNU Make](https://www.gnu.org/software/make/manual/html_node/Rule-Introduction.html#Rule-Introduction)):
```
target : prerequisites
    recipe
```

The basic idea is: to 'make' a target, first verify the existence
of its prerequisites, then execute a recipe.

The value starts to become more apparent with just a slightly adjusted example:
```
target2 : target1
    recipe2

target1 :
    recipe1
```

In this case, `make target2` will check for the existence of target1.  If
target1 exists, then 'make' will execute recipe2.  If target1 does not exist,
'make' will notice that target1 is *itself* a target.  Since target1 has no
prerequisites, it will execute recipe1 and thereby generate target1.  Then,
with target2's prerequisites satisfied, it will execute recipe2.

In this way it is possible to specify a detailed protocol to produce a
particular target.  When multiple dependencies exist, 'make' can keep track of
which ones have been updated, and how they affect the rest of the target
hierarchy, and rebuild other targets as needed.  When targets are well
specified, it is often possible to delete and regenerate a very specific output
in isolation.

### The Wider World of Build Automation

In fact, 'make' is just one of many things like 'make'.  It is one of the
oldest, and its Makefile syntax produces some of the most arcane project
descriptions.

Alternatives include Python's 'snakemake' and Google's 'bazel'
([don't call it 'blaze'](https://bazel.build/faq.html#whats-up-with-the-word-blaze-in-the-codebase)).

This project uses 'make' because it is very well-established.  Users
of this package most likely will not need to undertake any special steps
to install it, nor does the package itself need to make any provision for
such installation.  The downside is notoriously arcane Makefile syntax, but
that is mostly a burden for project contributors rather than users.  Without
caring about the actual contents of the Makefile, users can easily
understand and execute an instruction like this:

`make /opt/dnfa_genfiles/data/Analysis/Samsort/test2_S3_L004Aligned.sorted.bam`.

If you consider using 'make' for one of your own projects, good advice
would be: don't start by copying this project's Makefile and trying to
adjust it for your purposes.  Rather, just start out with some very simple
targets, then start learning and using automatic variables, one or two at
a time. Eventually you will find that your project has a gibberish Makefile
too, but to your surprise it will work, and to your even greater surprise
you will understand it. (Well, that's not a promise, on either count.  YMMV.)

## Unit Tests
(this section is under construction)

### A Primer on testthat
(this section is under construction)

### The World of Unit Test Systems
(this section is under construction)


## Documentation
(this section is under construction)

## A Primer on Roxygen2
(this section is under construction)

## The Wider World of Documentation Systems
(this section is under construction)
